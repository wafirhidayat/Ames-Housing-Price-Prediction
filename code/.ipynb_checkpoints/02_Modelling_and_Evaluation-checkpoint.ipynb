{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010ad738-7892-4000-b394-7fb2a37f18be",
   "metadata": {},
   "source": [
    "# Prediction Model\n",
    "\n",
    "We will be creating multiple regression models that will try and predict housing prices based on the features in our cleaned datasets. The models will be evaluated based on the root mean squared error (RMSE) of their predictions against the validation set. Once the best production model is found, the final model will then be retrained using the entire training dataset then its test set predictions will be submitted to Kaggle to determine the actual test score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ff78f-a143-4252-b490-38809ae98a3d",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Regression Modelling](#Regression-Modelling)\n",
    "- [Initial Kaggle Submission](#Initial-Kaggle-Submission)\n",
    "- [Model Improvement](#Model-Improvement)\n",
    "- [Final Kaggle Submission](#Final-Kaggle-Submission)\n",
    "- [Conclusions](#Conclusions)\n",
    "\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393f704-fbed-4b8f-bebb-9df2131e30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a00aea-8628-4f2e-b399-edcec9831957",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./datasets/training_model.csv')\n",
    "valid=pd.read_csv('./datasets/validation_model.csv')\n",
    "test=pd.read_csv('./datasets/test_kaggle.csv')\n",
    "ori_test=pd.read_csv('./datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7f31e-5af3-47c1-b729-16bad05168d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns='saleprice')\n",
    "X_valid = valid.drop(columns='saleprice')\n",
    "y_train = train['saleprice']\n",
    "y_valid = valid['saleprice']\n",
    "\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b444a-7c55-4c52-9b33-25efdac694f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the number of rows for each dataset\n",
    "print(f'Number of columns for X_train is {X_train.shape[1]}')\n",
    "print(f'Number of columns for X_valid is {X_valid.shape[1]}')\n",
    "print(f'Number of columns for X_train is {X_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4fc15-fa05-4ac0-867e-d11c14e58060",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ffd2b-a42e-47c8-900e-3149fc95cd34",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19bae1c-e70b-4b6d-a2f7-8c869da69525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "lr = LinearRegression()\n",
    "\n",
    "#cross-validation\n",
    "np.abs(cross_val_score(lr, X_train, y_train, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e902fa-6392-4fd1-9deb-da9369a479b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check r2 scores\n",
    "lr_scores = cross_val_score(lr, X_train, y_train, cv=3)\n",
    "\n",
    "print(lr_scores)\n",
    "lr_scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abb480-a9a5-40df-a2e6-71f828656f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation against validation set\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "np.sqrt(metrics.mean_squared_error(y_valid, lr.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae4ee9-8e5b-4c6c-8a98-b43396873b02",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7487f-f7f8-4e22-bf2c-5a082d79c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best alpha term\n",
    "r_alphas = np.linspace(.1, 10, 100)\n",
    "ridge_cv = RidgeCV(alphas=r_alphas)\n",
    "ridge_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c43ccb-aa5f-4d5d-bdd0-9dd07666ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a788435-89ff-4430-99c5-3539cea2af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "ridge = Ridge(alpha=ridge_cv.alpha_)\n",
    "\n",
    "#cross-validation\n",
    "np.abs(cross_val_score(ridge, X_train, y_train, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b16768-bbd3-4a5a-807c-be87576c9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check r2 scores\n",
    "ridge_scores = cross_val_score(ridge, X_train, y_train, cv=3)\n",
    "\n",
    "print(ridge_scores)\n",
    "ridge_scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d3a47-f798-41d6-8da3-31d7b8e39eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation against validation set\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "np.sqrt(metrics.mean_squared_error(y_valid, ridge.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53090a7-9b6d-4236-8e89-c80f487c4742",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5379180-af94-47eb-b658-c64a1b9e1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best alpha term\n",
    "lasso_cv = LassoCV(n_alphas=200)\n",
    "\n",
    "#fit model us\n",
    "lasso_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0a78d-8d17-4e16-bac7-018d1edbc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha\n",
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c91884-16f9-4e15-8d54-a70439aa0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "lasso = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\n",
    "\n",
    "#cross-validation\n",
    "np.abs(cross_val_score(lasso, X_train, y_train, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10bd64-56d9-450c-bd0f-2bbf2720e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check r2 scores\n",
    "lasso_scores = cross_val_score(lasso, X_train, y_train, cv=3)\n",
    "\n",
    "print(lasso_scores)\n",
    "lasso_scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fcb19-1c6d-4f13-9dbf-f57c718ecb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation against validation set\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "np.sqrt(metrics.mean_squared_error(y_valid, lasso.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663604e-7e11-49f7-b587-06d602d7648c",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df67a4-9f1b-4500-ab5b-0ee9b8bcb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best alpha term\n",
    "enet_alphas = np.linspace(0.5, 1.0, 100)# Return evenly spaced numbers over a specified interval\n",
    "enet_cv = ElasticNetCV(alphas=enet_alphas, cv=5) #l1_ratiofloat, default=0.5\n",
    "\n",
    "# Fit model using optimal alpha.\n",
    "enet_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09fbd5-937e-45b1-bfa9-2cbc1ae02b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best alpha\n",
    "enet_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e7abf-56b1-4917-8472-a3861d9b9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "enet = ElasticNet(alpha=enet_cv.alpha_)\n",
    "\n",
    "enet_scores = cross_val_score(lasso, X_train, y_train, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747112-70db-4402-b9e3-b3177b8c3467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation\n",
    "np.abs(cross_val_score(enet, X_train, y_train, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05382d2b-8e15-4b46-a2ed-2a82527d6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check r2 score\n",
    "enet_scores = cross_val_score(enet, X_train, y_train, cv=7)\n",
    "\n",
    "print(enet_scores)\n",
    "enet_scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86068371-c44a-4ef2-92ec-c4bf86e810e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation against validation set\n",
    "enet.fit(X_train, y_train)\n",
    "\n",
    "np.sqrt(metrics.mean_squared_error(y_valid, enet.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c06489-5c00-4893-a104-c7e3acda574e",
   "metadata": {},
   "source": [
    "\n",
    "### Initial Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e89927-5fbc-46cd-a124-e2d4e9e2b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_submission(preds, model_name):\n",
    "    \n",
    "    submission = pd.DataFrame(data=preds)\n",
    "    submission = pd.merge(ori_test['Id'], submission, left_index = True, right_index = True)\n",
    "    \n",
    "    submission.rename({'Id' : 'ID',\n",
    "                      0 : 'SalePrice'},\n",
    "                     inplace = True,\n",
    "                     axis = 1)\n",
    "    \n",
    "    submission.to_csv(f'./datasets/submission_{model_name}.csv', index=False)\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c49b57-9a76-4f0a-b003-c04b020f9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lr = lr.predict(X_test)\n",
    "kaggle_submission(test_preds_lr, 'linear_reg_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92bb75-e97a-4006-94ef-8a4286a4445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_ridge = ridge.predict(X_test)\n",
    "kaggle_submission(test_preds_ridge, 'ridge_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191efbe2-eddd-4833-bc26-8b23c8bd69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lasso = lasso.predict(X_test)\n",
    "kaggle_submission(test_preds_lasso, 'lasso_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a8b16-69ad-4b33-a21e-592a4117c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_enet = enet.predict(X_test)\n",
    "kaggle_submission(test_preds_enet, 'enet_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a03608-a79c-48dd-b1ac-a7d2a7864d8a",
   "metadata": {},
   "source": [
    "We run through our datasets through 4 regression models; Linear, Ridge, Lasso and ElasticNet.\n",
    "\n",
    "\n",
    "|Model | Penalty | α | Train Score | Cross-Validation Score | Kaggle Score (Public) |\n",
    "|---|---|---|---|---|---|\n",
    "| Linear Regression | - | - | -2.0611 | 1.6718e^14 | 7.0848e^14 |\n",
    "| Ridge Regression | L2 | 4.5 | 0.906 | 24810.7735 | 23432.86209 |\n",
    "| Lasso Regression | L1 | 65.3995 | 0.9075 | 24670.8261 | 23531.36260 |\n",
    "| ElasticNet Regression | L1+L2 | 0.5 | 0.8884 | 26979.0956 | 28275.50000 |\n",
    "\n",
    "For Linear, the scores show that the model is not useful in this dataset, especially when there are 191 features, after pre-processing, and Linear Regression does not regularization and so, the 'noise' that may be present in the model is still taken into consideration.\n",
    "\n",
    "As for the Ridge and Lasso models, they had the better scores across the scoring metric out of the four models, with Lasso being slightly better. They also scored well on Kaggle upon the first submission on the website. The main reason for this is that these regressions regularize the models and shrink the regression coefficients. Features that are deemed to be unimportant in contributing to the predictive value of the model are given smaller coefficients. Thus, the models tend to have lower variance and can generalise to new data better. Even though both ridge and lasso saw a significant result and prediction score, lasso saw a best score among the three regression models. This is because Lasso shrinks the irrelevant features to zero and allows the model for a better prediction model.\n",
    "\n",
    "Despite the good results, we will not stop here. There are 191 features in the dataset (58 before OneHot encoding) and with such a high number of features, the model may interpret these features differently. Therefore, to improve the model, we must reduce the complexity of it. Even with regularization, there may still be 'noise' in the model which may affect the model predictability.\n",
    "\n",
    "We will seek to reduce number of features through recursive feature elimination [(RFE)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) which select features by recursively considering smaller and smaller sets of features and filter those with the higher correlation with `'saleprice'`.\n",
    "\n",
    "Following which, hyperparamter tuning will be done on the reduced features selected by RFE to see if any improvements are seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20e41c-3187-4e29-b96f-582a1c23b8cb",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd31da-9181-46ee-aad6-df929795bbe8",
   "metadata": {},
   "source": [
    "### Feature Selection by RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92542-5a2e-4018-ab64-daac381ec154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using similar alpha for ease of computation\n",
    "model = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\n",
    "rfe = RFE(model, n_features_to_select=50)\n",
    "\n",
    "# Fitting to training data\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Tabulating RFE results\n",
    "rfe_results = [np.array(X_train.columns), rfe.ranking_]\n",
    "rfe_results_df = pd.DataFrame(rfe_results).T\n",
    "rfe_results_df.columns = ['Feature', 'RFE Ranking']\n",
    "\n",
    "# Finding features used by lasso (1 means feature was used)\n",
    "rfe_lasso_features = rfe_results_df.loc[rfe_results_df['RFE Ranking'] == 1, 'Feature'].tolist()\n",
    "\n",
    "#filtering the top features by RFE\n",
    "X_train_reduced = X_train[rfe_lasso_features]\n",
    "\n",
    "#Instantiate and fit\n",
    "lasso_cv_reduced = LassoCV(n_alphas=100, max_iter=10000)\n",
    "lasso_cv_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "#modelling\n",
    "lasso_reduced = Lasso(alpha=lasso_cv_reduced.alpha_, max_iter=10000)\n",
    "\n",
    "print(f'CVS for top 50 is : {np.abs(cross_val_score(lasso_reduced, X_train_reduced, y_train, scoring=\"neg_root_mean_squared_error\").mean())}')\n",
    "print(f'R2 cross for top 50 is {(cross_val_score(lasso_reduced, X_train_reduced, y_train, cv=3)).mean()}')\n",
    "\n",
    "lasso_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "print(f'Validation score for top 50 is : {np.sqrt(metrics.mean_squared_error(y_valid, lasso_reduced.predict(X_valid[rfe_lasso_features])))}')\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed50cc-bf46-48c5-ac7a-5d4d061f07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reduced = Lasso(alpha=lasso_cv_reduced.alpha_, max_iter=10000)\n",
    "lasso_reduced.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba94d5a-652b-4213-bbd7-b864f4fbb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting for X_test for the top 50 features for lasso\n",
    "test_preds_lasso_reduced = lasso_reduced.predict(X_test[rfe_lasso_features])\n",
    "kaggle_submission(test_preds_lasso_reduced, 'lasso_02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbe33f-3608-4eb5-b2d8-33599c67ff96",
   "metadata": {},
   "source": [
    "Based on our top 50 features based on RFE, the new Kaggle scores is shown to be a no improvement, despite having an improved scores on our model scoring metrics.\n",
    "Kaggle score was **23643.55423**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7ad93-048b-484d-8fbc-c01bca46131a",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b87473-a847-4840-80d3-1c3c7e8d7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso \n",
    "lasso_reduced_hyper = Lasso()\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "lasso_params = {'alpha':[0.005, 0.02, 0.03, 0.05, 0.06]}\n",
    "# define search\n",
    "search = GridSearchCV(lasso_reduced_hyper , lasso_params, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train[rfe_lasso_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468492f-6330-4bea-a4d0-126da05eddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05a683-ba87-48fc-afcb-7fc8c61a5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31468737-729c-488e-b4e9-849a44194859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "lasso_reduced_hyper = Lasso(alpha=0.06, max_iter=50000)\n",
    "\n",
    "#check r2 score\n",
    "print((cross_val_score(lasso_reduced_hyper , X_train[rfe_lasso_features], y_train)).mean())\n",
    "\n",
    "#cross-validation\n",
    "np.abs(cross_val_score(lasso_reduced_hyper , X_train[rfe_lasso_features], y_train, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab149a-afa0-468f-a78a-2967ce4caec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reduced_hyper.fit(X_train[rfe_lasso_features], y_train)\n",
    "test_preds_lasso_hyper = lasso_reduced_hyper.predict(X_test[rfe_lasso_features])\n",
    "\n",
    "#making kaggle submission dataset\n",
    "kaggle_submission(test_preds_lasso_hyper , 'lasso_03')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393d1a1-3a7a-4f88-ab0d-18ce5a7029ca",
   "metadata": {},
   "source": [
    "Based on our top 50 features based on RFE and after hyperparameter tuning, the new Kaggle scores is shown to be a no improvement as well, despite having an improved scores on our model scoring metrics.\n",
    "Kaggle score was **23605.50435**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9231a9d-768c-433b-baf1-e6665a08d4d9",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## Conclusion\n",
    "\n",
    "The final production model used was the Lasso Regression Model with 50 features selected using Recursive Feature Elimination (Lasso Regression). Even though it showed good metrics scoring result after validating with our valid dataset, the final Kaggle score showed no improvement, especially after the hyperparameter tuning process. This is likely due to the fact that CV models were already used initially to find the best alpha to fit our respective model. Thus, our initial model showed the best result, albeit only a slight change in scores after RFE filtering. \n",
    "\n",
    "The final tabulation of the models are as below:\n",
    "\n",
    "| Model | Penalty | α | Train Score | Cross-Validation Score | Kaggle Score (Public) |\n",
    "|---|---|---|---|---|---|\n",
    "| Linear Regression | - | - | -2.0611 | 1.6718e^14 | 7.0848e^14 |\n",
    "| Ridge Regression | L2 | 4.5 | 0.906 | 24810.7735 | 23429.53810 |\n",
    "| Lasso Regression | L1 | 65.3995 | 0.9075 | 24670.8261 | 23531.36260 |\n",
    "| ElasticNet Regression | L1+L2 | 0.5 | 0.8884 | 26979.0956 | 28275.50000 |\n",
    "| Lasso Regression (after RFE) | L1 | 0.5 | 0.913 | 24019.6993 | 23643.55423 |\n",
    "| Lasso Regression (after hyperparameter tuning) | L1 | 0.06 | 0.915 | 23582.1916 | 23932.42010 |\n",
    "\n",
    "From the Kaggle scores, it seems that Ridge was the slightly better model in predicting the saleprice of property on the test dataset, however Lasso seemed to performed better during the modelling stage. Albeit the scores were only off by a few 100 points, both Ridge and Lasso did a respectable job in prediction of the test dataset. \n",
    "\n",
    "Our final best Kaggle score was **23531**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b1ecc-82f7-4845-a1ef-a7ee5d4d5d2a",
   "metadata": {},
   "source": [
    "### Interpreting Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4f8eb-1dfe-41df-b6f0-921938ab856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reduced.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65489c1d-845c-44a5-aec9-41472b981c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reduced.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723f14c-892d-4e23-b4f5-0998a7a2a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataframe with coefficients of the reduced lasso features based on RFE \n",
    "coefs = pd.DataFrame([rfe_lasso_features, lasso_reduced.coef_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b2dec-15fa-446e-aa75-974cc8243f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.columns = ['feature', 'Coefficient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43785d80-af9b-4063-bdef-ea57d71d0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 features with positive coefficient\n",
    "coefs.sort_values(by='Coefficient', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98651597-b590-4c64-8233-92cfb473c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 features with negative coefficient\n",
    "coefs.sort_values(by='Coefficient', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2721152-2d57-4cd8-9147-a19661a36c7c",
   "metadata": {},
   "source": [
    "Feature coefficients can be simply interpreted as `Bx` in the linear regression equation: `Y = B0 + B1X1 + ... + BxXx + e`\n",
    "`B0` represents the intercept while `Bx` represents the slope parameter. For example, `'property_age'` impacts on our model as follows: \n",
    "\n",
    "`'sale_price'` = -11325.42 + 168148.42(`'overall_qual'`)\n",
    "\n",
    "Meaning to say, a one unit increase in age of the property before it was sold would relate to a predicted decrease in sale price of $16,8148.42.\n",
    "\n",
    "From the coefficients, we see that generally neighbourhoods and area/size of property contributes the most in sale price. Generally, locations that are 'prime' will lead to an increase in sale price and same goes to the size of the property as it shows a positive . Conversely, the features that had the greatest negative effect on sale price were the age of the house and certain roof styles (Mansard). The older the age of property, the lower the sale price and the more type of 'less-quality' roof styles (that are least popular among homeowners), the lower the sale price as well. \n",
    "\n",
    "Therefore, we can answer our project statement whereby the features listed in the coefficient dataframe has highlighted several similarly-described features has an impact on saleprice of properties in Ames, Iowa; namely **neighbourhood/location, size & area of properties and building types** contributes the most to the housing market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d80681-e27c-48ea-8ab5-222c07922a13",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578280ef-07fb-400a-a8c9-1fd708c8c48e",
   "metadata": {},
   "source": [
    "Our model is able to predict housing prices relatively well. It can also be fitted based on the needs of the stakeholders. Our dataset does not have some features such as economic indicators, like  employment or wage growth of the area, and interest rates. Based on our outside research, more macro features has an impact on the housing price in the market. An increase in interest rate can influence one's ability to afford a home as the individual's budget will be more focused on managing to pay off additional interest rates (for example, for credit card or short-term loan). [(source)](https://www.opendoor.com/w/blog/factors-that-influence-home-value)\n",
    "\n",
    "Therefore, we can include such features to better predict housing market based on more macro features, if the needs of our stakeholders require it to. In this case, our stakeholders are catered more to homeowners that may want to find ways to help alleviate their position in selling their properties at a higher price as the features describe more on the description of the property itself. If our stakeholders are more of real estate developers or investors, whose sole purpose is to see intrinsic value due to long-term price appreciation [(source)](https://www.investopedia.com/articles/investing/110614/most-important-factors-investing-real-estate.asp#:~:text=Expected%20cash%20flow%20from%20rental,to%20get%20a%20better%20price), then a more macro view and data is required in which our model is able to run prediction. \n",
    "\n",
    "Even with added features, our model will still be able to instantiate them and evaluate if the features are deemed significant in its predictive power. Especially with Lasso Regression Model, it can sieve out noise better by zeroing the coefficient. With proper datasets and training, we can recommend that the model be trained in more macro-related features to boost its predictive power in the housing market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad762c-ae18-4ef1-a56d-a3a3cac198c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
